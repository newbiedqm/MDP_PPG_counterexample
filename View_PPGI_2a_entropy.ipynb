{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projected Policy Gradient method (PPG) and Projected Policy Gradient with Interpoliation (PPGI) for the 2s2a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def project_onto_simplex(probabilities):\n",
    "    sorted_probs = -np.sort(-probabilities)\n",
    "    cumsum_probs = np.cumsum(sorted_probs)\n",
    "    rho = np.where(sorted_probs > (cumsum_probs - 1) / (np.arange(len(probabilities)) + 1))[0][-1]\n",
    "    theta = (cumsum_probs[rho] - 1) / (rho + 1)\n",
    "    projection = np.maximum(probabilities - theta, 0)\n",
    "    return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用矩阵形式求 discounted state occupancy measure, 这里简便起见将它乘上 1/(1 - gamma) ，并用它求出 V 和 Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_d(states, actions, policy, transition_probs, gamma):\n",
    "    \"\"\"\n",
    "    Compute d_{s}^{pi}(s') for all s, s' using the direct matrix form of discounted visitation frequency.\n",
    "\n",
    "    Args:\n",
    "        states: List of states.\n",
    "        actions: List of actions.\n",
    "        policy: Current policy (shape: [num_states, num_actions]).\n",
    "        transition_probs: Transition probabilities (shape: [num_states, num_actions, num_states]).\n",
    "        gamma: Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        d_s_pi: Discounted visitation frequencies (shape: [num_states, num_states]).\n",
    "                d_s_pi[s, s'] represents d_{s}^{pi}(s').\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_actions = len(actions)\n",
    "\n",
    "    # 构建策略下的转移矩阵 P_pi\n",
    "    P_pi = np.zeros((num_states, num_states))\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            P_pi[s, :] += policy[s, a] * transition_probs[s, a, :]\n",
    "\n",
    "    # 构建单位矩阵 I\n",
    "    I = np.eye(num_states)\n",
    "\n",
    "    # 计算 (I - gamma * P_pi) 矩阵\n",
    "    A = I - gamma * P_pi\n",
    "\n",
    "    # 检查矩阵是否可逆\n",
    "    if np.linalg.matrix_rank(A) < num_states:\n",
    "        raise ValueError(\"矩阵 (I - gamma * P_pi) 不可逆，可能需要调整折扣因子 gamma 或检查转移概率。\")\n",
    "\n",
    "    # 计算 D = (I - gamma * P_pi)^(-1)\n",
    "    D = np.linalg.inv(A)\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "def compute_v_q_from_d(states, actions, D, policy, rewards, gamma):\n",
    "    \"\"\"\n",
    "    Compute V and Q from d_{s}^{pi}(s').\n",
    "\n",
    "    Args:\n",
    "        states: List of states.\n",
    "        actions: List of actions.\n",
    "        D: Discounted visitation frequencies (shape: [num_states, num_states]).\n",
    "        policy: Current policy (shape: [num_states, num_actions]).\n",
    "        rewards: Reward function (shape: [num_states, num_actions]).\n",
    "        gamma: Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        V: Value function (shape: [num_states]).\n",
    "        Q: Action-value function (shape: [num_states, num_actions]).\n",
    "    \"\"\"\n",
    "    num_states = len(states)\n",
    "    num_actions = len(actions)\n",
    "    V = np.zeros(num_states)\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "    # Compute V(s) using D\n",
    "    for s in range(num_states):\n",
    "        V[s] = np.sum([\n",
    "            D[s, s_prime] * np.sum([\n",
    "                policy[s_prime, a] * rewards[s_prime, a]\n",
    "                for a in range(num_actions)\n",
    "            ])\n",
    "            for s_prime in range(num_states)\n",
    "        ])\n",
    "\n",
    "    # Compute Q(s, a) using the Bellman equation\n",
    "    for s in range(num_states):\n",
    "        for a in range(num_actions):\n",
    "            Q[s, a] = rewards[s, a] + gamma * np.sum([\n",
    "                transition_probs[s, a, s_prime] * V[s_prime]\n",
    "                for s_prime in range(num_states)\n",
    "            ])\n",
    "\n",
    "    return V, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy_joint_trajectory(policy_trajectory, eta, alpha):\n",
    "    \n",
    "    # Extract coordinates for π(s0, a0) and π(s1, a0)\n",
    "    x_coords = [policy[0, 0] for policy in policy_trajectory]  # π(s0, a0)\n",
    "    y_coords = [policy[1, 0] for policy in policy_trajectory]  # π(s1, a0)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Policy Trajectory for π(s0, a0) vs π(s1, a0)\")\n",
    "\n",
    "    # Plot the trajectory\n",
    "    plt.plot(x_coords, y_coords, '-o', label=\"Policy Trajectory\", alpha=0.8)\n",
    "\n",
    "    # Mark the initial point with a different color and label\n",
    "    plt.scatter(x_coords[0], y_coords[0], color='green', s=100, label=\"Initial Point\")\n",
    "    plt.text(x_coords[0], y_coords[0] + 0.02, \"Initial\", color='green', fontsize=10, ha='center')\n",
    "\n",
    "    # Mark the final point with a different color and label\n",
    "    plt.scatter(x_coords[-1], y_coords[-1], color='red', s=100, label=\"Final Point\")\n",
    "    plt.text(x_coords[-1], y_coords[-1] + 0.02, \"Final\", color='red', fontsize=10, ha='center')\n",
    "\n",
    "    # Add labels for the axes\n",
    "    plt.xlabel(\"π(s0, a0)\")\n",
    "    plt.ylabel(\"π(s1, a0)\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the additional information about parameters a and b\n",
    "    plt.text(0.5, 1.10, f\"eta={eta}, alpha={alpha}\", fontsize=12, ha='center', transform=plt.gca().transAxes)\n",
    "\n",
    "    # Add a legend\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projected_policy_gradient_interpolation(states, actions, transition_probs, rewards, gamma, rho, policy_init, eta, n_iter, alpha, tau):\n",
    "    policy = policy_init.copy()\n",
    "    policy_tar = policy_init.copy()  # Target policy for interpolation step\n",
    "    V_history = []\n",
    "    Q_history = []\n",
    "    discounted_rewards = []  # Store the discounted rewards\n",
    "    policy_trajectory = [policy.copy()]  # Track the trajectory of policies\n",
    "\n",
    "    for k in range(n_iter):\n",
    "        # Compute d_{s}^{pi}(s') using Monte Carlo\n",
    "        d_s_pi = compute_d(states, actions, policy, transition_probs, gamma)\n",
    "\n",
    "        # Compute V and Q using d_{s}^{pi}\n",
    "        V, Q = compute_v_q_from_d(states, actions, d_s_pi, policy, rewards, gamma)\n",
    "        V_history.append(V)\n",
    "        Q_history.append(Q)\n",
    "\n",
    "        # Compute total discounted reward\n",
    "        discounted_reward = np.dot(rho, V)  # Use initial state distribution rho to compute reward\n",
    "        discounted_rewards.append(discounted_reward)\n",
    "        \"\"\"\n",
    "        # Output current values and policy\n",
    "        print(f\"Iteration {k}:\")\n",
    "        print(\"Value Function V:\")\n",
    "        print(V)\n",
    "        print(\"Action-Value Function Q:\")\n",
    "        print(Q)\n",
    "        print(\"Policy π:\")\n",
    "        print(policy)\n",
    "        print(\"State Visitation Frequencies d_s_pi:\")\n",
    "        print(d_s_pi)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Check termination conditions\n",
    "        if k > 0:\n",
    "            # Condition 1: Check if |V_{s0}^{k+1} - V_{s0}^k| < 1e-6\n",
    "            if abs(V[0] - V_history[-2][0]) < 1e-8:\n",
    "                # Condition 2: Check if |π^{k+1}(s, a) - π^{k}(s, a)| < 1e-6 for all (s, a)\n",
    "                if np.all(np.abs(policy - policy_trajectory[-1]) < 1e-12):\n",
    "                    print(\"Termination conditions met.\")\n",
    "                    break\n",
    "        \n",
    "        # Compute policy gradient\n",
    "        gradient = np.zeros_like(policy)\n",
    "        for s in range(len(states)):\n",
    "            for a in range(len(actions)):\n",
    "                gradient[s, a] = d_s_pi[0, s] * Q[s, a] + tau * np.divide(1, policy[s, a] + 1e-16) # initial state distribution rho = [1,0,0]\n",
    "        \n",
    "        # Policy update\n",
    "        policy_tar = policy + eta * gradient\n",
    "        \n",
    "        # Projection onto simplex\n",
    "        for s in range(len(states)):\n",
    "            policy_tar[s] = project_onto_simplex(policy_tar[s])\n",
    "            if np.isnan(policy[s]).any():\n",
    "                raise ValueError(f\"Policy contains NaN after projection at state {s}\")\n",
    "        \n",
    "        # Interpolation step\n",
    "        policy = (1 - alpha) * policy + alpha * policy_tar # alpha = 1 for no interpolation\n",
    "        policy_trajectory.append(policy.copy())  # Track updated policy\n",
    "    \n",
    "    return V_history, Q_history, discounted_rewards, policy_trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改后的函数，支持多个初始策略\n",
    "def save_policy_trajectories_to_pdf(output_pdf, eta_values, alpha_values, states, actions, transition_probs, rewards, gamma, rho, policy_inits, n_iter, tau):\n",
    "    \"\"\"\n",
    "    Generate and save policy trajectories for different eta, alpha, and initial policy combinations to a PDF file.\n",
    "\n",
    "    Args:\n",
    "        output_pdf (str): The path to the output PDF file.\n",
    "        eta_values (list): A list of eta values to iterate over.\n",
    "        alpha_values (list): A list of alpha values to iterate over.\n",
    "        states (list): List of states in the MDP.\n",
    "        actions (list): List of actions in the MDP.\n",
    "        transition_probs (ndarray): Transition probabilities (shape: [num_states, num_actions, num_states]).\n",
    "        rewards (ndarray): Rewards matrix (shape: [num_states, num_actions]).\n",
    "        gamma (float): Discount factor.\n",
    "        rho (ndarray): Initial state distribution.\n",
    "        policy_inits (list of ndarray): List of initial policies (shape: [num_states, num_actions]).\n",
    "        n_iter (int): Number of iterations for the policy gradient.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        # To store results for all combinations\n",
    "        results = []\n",
    "\n",
    "        # 颜色列表用于区分不同初始策略\n",
    "        colors = ['blue', 'green', 'red', 'purple', 'orange']\n",
    "        markers = ['o', 's', 'D', '^', 'v']  # 不同标记\n",
    "\n",
    "        for alpha in alpha_values:\n",
    "            for eta in eta_values:\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.title(f\"Policy Trajectories (eta={eta}, alpha={alpha}, tau={tau})\")\n",
    "\n",
    "                # 对每个初始策略运行算法并绘图\n",
    "                for idx, policy_init in enumerate(policy_inits):\n",
    "                    # 运行算法\n",
    "                    V_history, Q_history, discounted_rewards, policy_trajectory = projected_policy_gradient_interpolation(\n",
    "                        states, actions, transition_probs, rewards, gamma, rho, policy_init, eta, n_iter, alpha, tau\n",
    "                    )\n",
    "\n",
    "                    # 提取坐标\n",
    "                    x_coords = [policy[0, 0] for policy in policy_trajectory]\n",
    "                    y_coords = [policy[1, 0] for policy in policy_trajectory]\n",
    "\n",
    "                    # 生成策略标签（基于初始值）\n",
    "                    init_label = f\"Init: s0={policy_init[0,0]:.2f}, s1={policy_init[1,0]:.2f}\"\n",
    "\n",
    "                    # 绘制轨迹\n",
    "                    plt.plot(x_coords, y_coords, '-', \n",
    "                            color=colors[idx % len(colors)],\n",
    "                            marker=markers[idx % len(markers)],\n",
    "                            markersize=4,\n",
    "                            linewidth=1.5,\n",
    "                            alpha=0.8,\n",
    "                            label=init_label)\n",
    "\n",
    "                    # 标记初始点和终点\n",
    "                    plt.scatter(x_coords[0], y_coords[0], \n",
    "                               color=colors[idx % len(colors)],\n",
    "                               s=100, \n",
    "                               edgecolor='black',\n",
    "                               zorder=10)\n",
    "                    plt.scatter(x_coords[-1], y_coords[-1],\n",
    "                               color=colors[idx % len(colors)],\n",
    "                               s=200,\n",
    "                               marker='X',  # 用X标记终点\n",
    "                               edgecolor='black',\n",
    "                               zorder=10)\n",
    "\n",
    "                # 统一设置图表属性\n",
    "                plt.xlabel(\"π(s0, a0)\")\n",
    "                plt.ylabel(\"π(s1, a0)\")\n",
    "                plt.xlim(0, 1)\n",
    "                plt.ylim(0, 1)\n",
    "                plt.grid(True)\n",
    "                plt.legend(loc='upper left', bbox_to_anchor=(1, 1))  # 图例放在右侧\n",
    "\n",
    "                # 保存当前组合的图表\n",
    "                pdf.savefig(bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "        # 添加总结页\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Summary of All Experiments\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        summary_text = \"Experiment Parameters:\\n\"\n",
    "        summary_text += f\"- States: {states}\\n\"\n",
    "        summary_text += f\"- Actions: {actions}\\n\"\n",
    "        summary_text += f\"- Gamma: {gamma}\\n\"\n",
    "        summary_text += \"\\nInitial Policies:\\n\"\n",
    "        for idx, policy in enumerate(policy_inits):\n",
    "            summary_text += f\"Policy {idx+1}:\\n{np.round(policy, 2)}\\n\\n\"\n",
    "        \n",
    "        plt.text(0.1, 0.5, summary_text, ha='left', va='center', fontsize=10)\n",
    "        pdf.savefig()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面直接验证论文中的MDP反例1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termination conditions met.\n",
      "Termination conditions met.\n",
      "Termination conditions met.\n",
      "All policy trajectories and results have been saved to multi_init_policy_trajectories_0.pdf.\n"
     ]
    }
   ],
   "source": [
    "# 修改后的主程序部分\n",
    "if __name__ == \"__main__\":\n",
    "    # MDP参数保持不变\n",
    "    states = [0, 1, 2]\n",
    "    actions = [0, 1]\n",
    "    transition_probs = np.array([\n",
    "        [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]],\n",
    "        [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
    "        [[0.0, 0.0, 1.0], [0.0, 0.0, 1.0]]\n",
    "    ])\n",
    "    rewards = np.array([\n",
    "        [-1, -1],\n",
    "        [-1.6, -1.6],\n",
    "        [0, 0]\n",
    "    ])\n",
    "    gamma = 0.5\n",
    "    rho = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "    # 定义多个不同的初始策略\n",
    "    policy_inits = [\n",
    "        # 初始策略1 (原策略)\n",
    "        np.array([\n",
    "            [0.96, 0.04],\n",
    "            [0.92, 0.08],\n",
    "            [0.50, 0.50]\n",
    "        ]),\n",
    "        # 初始策略2 (更均匀的分布)\n",
    "        np.array([\n",
    "            [0.98, 0.02],\n",
    "            [0.80, 0.20],\n",
    "            [0.50, 0.50]\n",
    "        ]),\n",
    "        # 初始策略3 (偏向另一个动作)\n",
    "        np.array([\n",
    "            [0.99, 0.01],\n",
    "            [0.70, 0.30],\n",
    "            [0.50, 0.50]\n",
    "        ])\n",
    "    ]\n",
    "\n",
    "    n_iter = 200000\n",
    "    eta_values = [0.01]\n",
    "    alpha_values = [0.5]\n",
    "    tau = 0\n",
    "\n",
    "    # 输出PDF文件\n",
    "    output_pdf = \"multi_init_policy_trajectories_0.pdf\"\n",
    "\n",
    "    # 调用修改后的函数\n",
    "    save_policy_trajectories_to_pdf(\n",
    "        output_pdf=output_pdf,\n",
    "        eta_values=eta_values,\n",
    "        alpha_values=alpha_values,\n",
    "        states=states,\n",
    "        actions=actions,\n",
    "        transition_probs=transition_probs,\n",
    "        rewards=rewards,\n",
    "        gamma=gamma,\n",
    "        rho=rho,\n",
    "        policy_inits=policy_inits,\n",
    "        n_iter=n_iter,\n",
    "        tau=tau\n",
    "    )\n",
    "\n",
    "    print(f\"All policy trajectories and results have been saved to {output_pdf}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
